[{"content":"A Project That Found Me 🚀 While working as a Full-Stack Developer at OpenNLP Labs, I was deeply involved in projects that blended AI, healthcare, and human cognition 🧠. One of my past experiences involved NLP and computer vision, areas that had always fascinated me.\nOne day, I was told that openNLP Labs has more projects with an opportunity to work on something even more impactful—a project that could help Alzheimer’s patients preserve their memories and track cognitive changes using AI. It was none other than the Reteena Project, where we aimed to develop adaptive digital twins using LLMs and VLMs.\nThe Vision Behind Reteena 🏥 The core idea was to create a personalized AI assistant that could:\nEnhance memory recall by structuring past conversations and interactions 📝 Analyze cognitive changes over time through AI-driven assessments 📊 Support families and caregivers by providing insights into the patient’s condition ❤️ Additionally, we worked on a web platform that could enhance low-field MRI scans, making early Alzheimer’s diagnosis more affordable for a larger population. The Impact and Recognition 🌟 The Project was already successful, our work gained recognition:\n✅ Accepted at the IEEE BigData Undergraduate \u0026amp; High School Symposium 📃 ✅ Partnered with Microsoft Founders Hub, gaining access to Azure Cloud Services, developer tools, and expert mentorship ☁️ A Full Circle Moment 🎯 Looking back, this project perfectly aligned with my interests—AI, human cognition, and psychology. It was a reminder that every skill and experience we gain along the way eventually leads us to opportunities where we can make a real difference.\nReteena wasn’t just another AI project—it was a mission to bring hope and support to those battling memory loss. And I’m grateful to be a part of it.\nTo know more check our my LinkedIn\n","permalink":"http://localhost:1313/experience/reteena/","summary":"\u003ch1 id=\"a-project-that-found-me-\"\u003eA Project That Found Me 🚀\u003c/h1\u003e\n\u003cp\u003eWhile working as a Full-Stack Developer at OpenNLP Labs, I was deeply involved in projects that blended AI, healthcare, and human cognition 🧠. One of my past experiences involved NLP and computer vision, areas that had always fascinated me.\u003c/p\u003e\n\u003cp\u003eOne day, I was told that openNLP Labs has more projects with an opportunity to work on something even more impactful—a project that could help Alzheimer’s patients preserve their memories and track cognitive changes using AI. It was none other than the Reteena Project, where we aimed to develop adaptive digital twins using LLMs and VLMs.\u003c/p\u003e","title":"Machine Learning Intern"},{"content":"From Curiosity to Opportunity: My Journey to OpenNLP Labs It all started with my deep fascination for AI and human-computer interaction. I had always been eager to explore how machines understand and process language, which led me to dive deep into Machine Learning (ML) and Natural Language Processing (NLP).\nThe Search for Opportunities 🚀 During my research internships, I constantly worked on projects that blended AI, psychology, and data science. But I wanted something more—an opportunity where I could apply my ML knowledge in a real-world setting while also honing my software development skills.\nOne day, while scrolling through job listings and LinkedIn posts, I came across an opening for a Full-Stack Developer and ML Intern at OpenNLP Labs. The role seemed like a perfect match—it combined my passion for ML, NLP, and software development. Without hesitation, I prepared my resume and a cover letter, showcasing my previous experiences in AI research, full-stack development, and cognitive science projects.\nThe Unexpected Interview Call 📞 A week later, I received an email from OpenNLP Labs—I had been shortlisted for an interview! 🎉 While I was excited, I also felt a wave of nervousness. Unlike my previous research roles, this was an industry-oriented position that required both ML expertise and hands-on full-stack development skills.\nThe interview process was rigorous. I was asked about:\nMy experience with ML and NLP models 🤖 My knowledge of front-end and back-end development 💻 How I approach problem-solving in AI-driven applications 🔍 What helped me stand out was my ability to bridge the gap between AI and software engineering—a skill I had cultivated through my various projects, from building handwriting assessment apps to working on Digital Twin models for Alzheimer’s patients.\nThe Offer Letter and the Journey Begins! 🎊 A few days after the interview, I received an offer letter from OpenNLP Labs! I was officially joining as a Full-Stack Developer and ML Intern—a role that would allow me to work on real-world AI applications, optimize NLP models, and develop scalable web applications.\nLooking back, it all started with curiosity, perseverance, and a willingness to explore multiple domains. This opportunity not only reinforced my technical skills but also showed me the power of combining AI with software development to create impactful solutions.\nAnd so, my journey at OpenNLP Labs began—a thrilling adventure at the intersection of AI and engineering! 🚀\nTo know more check our my LinkedIn\n","permalink":"http://localhost:1313/experience/opennlp_labs/","summary":"\u003ch2 id=\"from-curiosity-to-opportunity-my-journey-to-opennlp-labs\"\u003eFrom Curiosity to Opportunity: My Journey to OpenNLP Labs\u003c/h2\u003e\n\u003cp\u003eIt all started with my deep fascination for AI and human-computer interaction. I had always been eager to explore how machines understand and process language, which led me to dive deep into Machine Learning (ML) and Natural Language Processing (NLP).\u003c/p\u003e\n\u003cp\u003eThe Search for Opportunities 🚀\nDuring my research internships, I constantly worked on projects that blended AI, psychology, and data science. But I wanted something more—an opportunity where I could apply my ML knowledge in a real-world setting while also honing my software development skills.\u003c/p\u003e","title":"Full-Stack Developer/Machine Learning Intern"},{"content":"My Experience I was fortunate to have the opportunity to work at one of the world’s top university labs—CMU (Carnegie Mellon University) 🏛️✨. For me, CMU felt like a gateway to heaven—a place where I could immerse myself in cutting-edge research and engage with people deeply involved in AI and human biology 🧠🤖, which was truly the cherry on top of the cake 🍰.\nAfter my semester exams in late summer, I discovered that some research labs hire undergraduate students remotely as Research Assistants (RA) under PhD candidates. Excited by this, I came across a Computational Biology Lab at CMU 🧬🔬. Without hesitation, I filled out the application form and emailed them about my experience.\nI was thrilled to be selected as an RA under Prof. Min Xu, working under his PhD students on Computer Vision projects 👨‍💻📊. This experience was truly transformative and fueled my passion for interdisciplinary research! 🚀\nTo know more check our my LinkedIn\n","permalink":"http://localhost:1313/experience/ra_cmu/","summary":"\u003ch1 id=\"my-experience\"\u003eMy Experience\u003c/h1\u003e\n\u003cp\u003eI was fortunate to have the opportunity to work at one of the world’s top university labs—CMU (Carnegie Mellon University) 🏛️✨. For me, CMU felt like a gateway to heaven—a place where I could immerse myself in cutting-edge research and engage with people deeply involved in AI and human biology 🧠🤖, which was truly the cherry on top of the cake 🍰.\u003c/p\u003e\n\u003cp\u003eAfter my semester exams in late summer, I discovered that some research labs hire undergraduate students remotely as Research Assistants (RA) under PhD candidates. Excited by this, I came across a Computational Biology Lab at CMU 🧬🔬. Without hesitation, I filled out the application form and emailed them about my experience.\u003c/p\u003e","title":"Research Assistant at Carnegie Mellon University"},{"content":"How did I able to get an Research Internship at a Hospital 🏥💡 Introduction Hi, ✋😃 thanks for stopping by and reading this blog! In this post, I’ll share my journey of how, as a first-year Psychology major and a second-year Computer Science major, I was able to land an internship at a hospital. Stay tuned to learn about the steps I took, the challenges I faced, and the strategies that helped me secure this opportunity!\nBackground 📖- Around mid-July, I became deeply obsessed with finding a role that aligned with both my interests and knowledge. I wanted to contribute to something meaningful and impactful, so I began researching opportunities. I explored how many people were seeking research roles and where I could fit in. As a dual major student, I was open to any research position related to human psychology and AI, eager to apply my skills in a way that truly made a difference. As I told you, I wanted to work on something impactful that would provide me with hands-on experience.\n🔎 My goals:✅ Gain practical research experience 🧠 ✅ Work on a project related to healthcare \u0026amp; data 📊 ✅ Enhance my analytical and problem-solving skills 🛠️ Applying for the Internship 📝 I was obsessed and eager to do some research work when I stumbled upon a LinkedIn post titled \u0026ldquo;Looking for RA\u0026rdquo; 🔍. Without a second thought, I jumped in 🚀.\nThey were looking for candidates with an educational background in psychology 🧠 and some knowledge of data analysis 📊. Luckily, I was confident in my skills! So, I applied for the internship by sending my resume and a cover letter via email ✉️.\nAfter a week of anxious waiting, I received an email saying that my resume was shortlisted 🎉, and I was invited for an interview!\nBefore that, I kept thinking, \u0026ldquo;No one is going to hire me… why would they?\u0026rdquo; 😔. But then I realized—there are so many opportunities out there 🌍!\nI still remember that I had an engineering semester exam the next day 🎓, so I was super nervous 😬. But somehow, they saw something in me that I couldn\u0026rsquo;t even see in myself 🤯.\nAs an introvert 🤐, I felt out of place at first—especially since I was the only guy on the team 👦💼. But this experience taught me so much and pushed me out of my comfort zone 🚀!\nWhat did I learn during my Research Internship at a Hospital 📊🔬 During my internship at a hospital, I learned how to work in a clinical setting. I also learned how to work with patients and how to conduct research. I learned how to analyze data. I also learned how to work with a team and how to communicate effectively with others.\nWorking as a RA to love for child/infant and study about them more I had the incredible opportunity to work on how infants and young children think and perceive the world 🧠👶. I was working under Dr. Madhavilatha Maganti-Kari at the Infant and Child Development – Emerging Minds Lab, focusing on early cognitive development.\nOur team successfully conducted eye-tracking trials for the MB2 project, studying infants up to 24 months 👀📊.\nWhat made this experience even more exciting was that I had never realized how fascinating and fun it could be to study infants and young children! 🤩✨\nLets First understand what is Child development? Child development stages refer to the different phases of growth and learning that children go through from infancy to adulthood. Various psychologists and theorists have proposed models to explain these stages. Here are some of the most well-known ones:\nJean Piaget\u0026rsquo;s Cognitive Development Theory (1896–1980) 🧠 Piaget proposed that children go through four stages of cognitive development, each with different ways of thinking and understanding the world. Sensorimotor Stage (0–2 years) 👶: Learning through sensory experiences and motor activities. Preoperational Stage (2–7 years) 🎭: Development of language and imagination but egocentric thinking. Concrete Operational Stage (7–11 years) 🔢: Logical thinking begins, understanding of conservation and classification. Formal Operational Stage (12+ years) 🤔: Abstract and hypothetical thinking develop. Erik Erikson\u0026rsquo;s Psychosocial Development Theory (1902–1994) ❤️: Erikson identified eight stages of psychosocial development, each marked by a key conflict that shapes personality. Some key stages include: Trust vs. Mistrust (0–1 year) 🤱: Learning to trust caregivers. Autonomy vs. Shame \u0026amp; Doubt (1–3 years) 🚶: Developing independence. Initiative vs. Guilt (3–6 years) 🎭: Taking initiative in activities. Industry vs. Inferiority (6–12 years) 🏫: Gaining competence in skills. Identity vs. Role Confusion (12–18 years) 🤯: Forming a sense of identity. Lev Vygotsky’s Sociocultural Theory (1896–1934) 🌍: Vygotsky emphasized the role of social interaction in cognitive development. His key concept was the Zone of Proximal Development (ZPD), which describes the gap between what a child can do alone and what they can achieve with guidance.\nSigmund Freud’s Psychosexual Development Theory (1856–1939)💭: Freud proposed that personality develops through five psychosexual stages, focusing on pleasure-seeking behaviors in different body areas (oral, anal, phallic, latency, and genital).\nLawrence Kohlberg’s Moral Development Theory (1927–1987) ⚖️: Kohlberg expanded on Piaget’s work and outlined three levels of moral reasoning:\nPreconventional (Childhood) 🚸: Obedience and self-interest. Conventional (Adolescence) 🏛️: Following social rules. Postconventional (Adulthood, if reached) ⚖️: Abstract ethical principles. Each of these theories provides a unique perspective on how children develop cognitively, emotionally, and socially\nTo know more check our my LinkedIn\n","permalink":"http://localhost:1313/experience/ra_ashoka/","summary":"\u003ch1 id=\"how-did-i-able-to-get-an-research-internship-at-a-hospital-\"\u003eHow did I able to get an Research Internship at a Hospital 🏥💡\u003c/h1\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eHi, ✋😃 thanks for stopping by and reading this blog! In this post, I’ll share my journey of how, as a first-year Psychology major and a second-year Computer Science major, I was able to land an internship at a hospital. Stay tuned to learn about the steps I took, the challenges I faced, and the strategies that helped me secure this opportunity!\u003c/p\u003e","title":"Research Assistant at Ashoka University"},{"content":"I am currently working as a Teaching Intern at Pehchaan The Street School, an NGO dedicated to providing education to underprivileged children 📚✨. My role involves teaching students from elementary to 9th grade, helping them build foundational skills in various subjects, and fostering curiosity and critical thinking 🧠💡. It’s an incredibly rewarding experience, as I get to make a real impact on young minds who otherwise have limited access to quality education. Teaching here has reinforced my belief that education is the most powerful tool for change. 🚀🎓\nTo know more check our my LinkedIn\n","permalink":"http://localhost:1313/experience/ngo/","summary":"\u003cp\u003eI am currently working as a Teaching Intern at Pehchaan The Street School, an NGO dedicated to providing education to underprivileged children 📚✨. My role involves teaching students from elementary to 9th grade, helping them build foundational skills in various subjects, and fostering curiosity and critical thinking 🧠💡. It’s an incredibly rewarding experience, as I get to make a real impact on young minds who otherwise have limited access to quality education. Teaching here has reinforced my belief that education is the most powerful tool for change. 🚀🎓\u003c/p\u003e","title":"Teaching Intern at a NGO"},{"content":"I had the opportunity to work as a Campus Ambassador at GeeksForGeeks, where I engaged in community building, event promotion, and student outreach. My role involved organizing workshops, spreading awareness about programs, and fostering engagement among peers. It helped me develop leadership, communication, and networking skills while contributing to a larger mission. 🚀\nTo know more check our my LinkedIn\n","permalink":"http://localhost:1313/experience/ca/","summary":"\u003cp\u003eI had the opportunity to work as a Campus Ambassador at GeeksForGeeks, where I engaged in community building, event promotion, and student outreach. My role involved organizing workshops, spreading awareness about programs, and fostering engagement among peers. It helped me develop leadership, communication, and networking skills while contributing to a larger mission. 🚀\u003c/p\u003e\n\u003cp\u003eTo know more check our my \u003ca href=\"https://www.linkedin.com/in/anurag-sharma-o7xd/\"\u003eLinkedIn\u003c/a\u003e\u003c/p\u003e","title":"Campus Ambassador Program"},{"content":"Check Out my Blog Website to know more:- Link\nMIT 9.13 The Human Brain, Spring 2019 Instructor: Nancy Kanwisher\nView the complete course\nLecture 1: Introduction Download the PDF of Lecture01 Prof. Kanwisher tells a true story to introduce the course, then covers the why, how, and what of studying the human brain and gives a course overview. {: .prompt-info}\nWatch the Lecture Video Lecture 2: Neuroanotomy Download the PDF Basic brief neuroanatomy review in preparation for dissection, including an introduction to the cortex, primary regions, and topographic maps. {: .prompt-info}\nWatch the Video Lecture Lecture 4 : Cognitive Neuroscience Methods I Download the PDF Introduction to methods in cognitive neuroscience including computation, behavior, fMRI, ERPs \u0026amp; MEG, neuropsychology patients, TMS, and intracranial recordings in humans and nonhuman primates. {: .prompt-info}\nWatch Video Lecture Lecture 5: Cognitive Neuroscience Methods II Download the PDF Methods in cognitive neuroscience continued. {: .prompt-info}\nWatch the Video Lecture Lecture 6: Introduction to the Human Brain Download the PDF This session reviews the last two lectures, face recognition, and explores types of experimental methods. {: .prompt-info}\nWatch the video Lecture Lecture 7: Category Selectivity, Controversies, and MVPA Download the PDF Covers controversies and alternative views of the ventral visual pathway, multiple voxel pattern analysis, and the two visual pathways. {: .prompt-info}\nWatch the Video Lecture here Lecture 8: Navigation I Download the PDF The functional organization of scene perception and navigation and the various brain structures that implement them. {: .prompt-info}\nWatch the Video Lecture Lecture 9: Navigation II Download the PDF Scene perception and navigation continued. {: .prompt-info}\nWatch Lecture_09 Lecture 10: Development, Nature \u0026amp; Nurture I Download the PDF This lecture examines how we think the cortex organizes in the brain over infancy and childhood, and the function of genes vs experience. {: .prompt-tip}\nWatch Video Lecture Lecture 11: Development, Nature \u0026amp; Nurture II (2018) Download the PDF Continues the discussion of genes vs experience on cortical organization, and whether the cortex can change in adulthood. {: .prompt-tip}\nWatch Video Lecture Lecture 13: Number Download the PDF Explores the nature of the human representation of number and how it is implemented in the brain. NOTE: Lecture 14: New Methods Applied to Number (student breakout groups—video not recorded) {: .prompt-info}\nWatch Video Lecture. Lecture 15: Hearing and Speech Download the PDF Humans use hearing in species-specific ways, for speech and music. Ongoing research is working out the functional organization of these and other human auditory skills. {: .prompt-info}\nWatch Video Lecture online Lecture 16: Music Download the PDF The functional organization of music in human beings. NOTE: Lecture 17: MEG Decoding and RSA (video not recorded) {: .prompt-info}\nWatch Lecture on \"Music\" Lecture 18: Language I Download the PDF Covers the basic organization of language in the brain and the long-standing question of the relationship between thought and language.\nNOTE: Lecture 17: MEG Decoding and RSA (video not recorded) NOTE: Lecture 19: Language II (class canceled—video not recorded) {: .prompt-info} Video Lecture Lecture 20: Theory of Mind \u0026amp; Mentalizing Download the PDF The ability of humans to think about what other people are thinking is implemented in brain regions highly specialized for this function alone.\nNOTE: Lecture 19: Language II (class canceled—video not recorded) {: .prompt-info}\nWatch Lecture online Lecture 21: Brain Networks Download the PDF Looks at the major white matter tracts in the human brain, predicting function and correlations between regions.\nNOTE: Lecture 22: Experimental Design (student breakout groups—video not recorded) NOTE: Lecture 23: Deep Networks (2021) (video will be added soon) {: .prompt-tip}\nWatch Video lectures Lecture 24: Attention and Awareness Download the PDF Looks at the differences in the mind and brain between perceptual information we are aware of versus information we are not. NOTE: Lecture 23: Deep Networks (2021) (video will be added soon) {: .prompt-tip}\nWatch Video Lecture online ","permalink":"http://localhost:1313/blog/human-ai/","summary":"\u003cp\u003eCheck Out my Blog Website to know more:- \u003ca href=\"https://anuragsharma5893.github.io/\"\u003eLink\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"mit-913-the-human-brain-spring-2019\"\u003eMIT 9.13 The Human Brain, Spring 2019\u003c/h2\u003e\n\u003cp\u003eInstructor: Nancy Kanwisher\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://ocw.mit.edu/9-13S19\"\u003eView the complete course\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.youtube.com/playlist?list=PLUl4u3cNGP60IKRN_pFptIBxeiMc0MCJP\"\u003e\u003cimg alt=\"Playlist\" loading=\"lazy\" src=\"https://img.shields.io/badge/YouTube-FF0000?style=for-the-badge\u0026logo=youtube\u0026logoColor=white\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"lecture-1-introduction\"\u003eLecture 1: Introduction\u003c/h1\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003ca href=\"/assets/Notes/MIT9_13S19_L01.pdf\"\u003eDownload the PDF of Lecture01\u003c/a\u003e\nProf. Kanwisher tells a true story to introduce the course, then covers the why, how, and what of studying the human brain and gives a course overview.\n{: .prompt-info}\u003c/p\u003e\u003c/blockquote\u003e\n\u003ciframe src=\"{{ site.baseurl }}/assets/Notes/MIT9_13S19_L01.pdf\" width=\"90%\" height=\"500px\"\u003e\u003c/iframe\u003e\nWatch the Lecture Video  \n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/ba-HMvDn_vU?si=Oj1QIXPjO_jeVGQS\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen\u003e\u003c/iframe\u003e\n\u003ch1 id=\"lecture-2-neuroanotomy\"\u003eLecture 2: Neuroanotomy\u003c/h1\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003ca href=\"/assets/Notes/MIT9_13S19_L02.pdf\"\u003eDownload the PDF\u003c/a\u003e\nBasic brief neuroanatomy review in preparation for dissection, including an introduction to the cortex, primary regions, and topographic maps.\n{: .prompt-info}\u003c/p\u003e","title":"Human Ai"},{"content":"🔹 Personal Blog Website A GitHub Pages-powered blog built using Jekyll, where I share my thoughts on AI, psychology, NLP, and technology. The blog serves as a platform to document my learning journey, research insights, and experiences in machine learning and human cognition. It is continuously updated with new articles, project write-ups, and technical deep dives.\nTech Stack: Jekyll, Markdown, GitHub Pages, HTML, CSS\n👉 Visit My Blog\n","permalink":"http://localhost:1313/project/blog/","summary":"\u003ch2 id=\"-personal-blog-website\"\u003e🔹 \u003cstrong\u003ePersonal Blog Website\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eA \u003cstrong\u003eGitHub Pages-powered blog\u003c/strong\u003e built using \u003cstrong\u003eJekyll\u003c/strong\u003e, where I share my thoughts on \u003cstrong\u003eAI, psychology, NLP, and technology\u003c/strong\u003e. The blog serves as a platform to document my learning journey, research insights, and experiences in \u003cstrong\u003emachine learning and human cognition\u003c/strong\u003e. It is continuously updated with \u003cstrong\u003enew articles, project write-ups, and technical deep dives\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eTech Stack:\u003c/strong\u003e Jekyll, Markdown, GitHub Pages, HTML, CSS\u003c/p\u003e\n\u003cp\u003e👉 \u003cstrong\u003e\u003ca href=\"https://anuragsharma5893.github.io/\"\u003eVisit My Blog\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e","title":"Blogs"},{"content":"LLM Model a game changer to the world of AI LLM_model This app is a dummy of the Big available LLM model using models like Ollama deepseek-r1:1.5b\n🧠 DeepSeek Code Companion Your AI-powered pair programmer with advanced debugging capabilities and code optimization features.\nFeatures 🚀 Multi-model support (DeepSeek, LLaVA, Llama3) 🔥 Real-time code debugging assistance 📝 Automatic code documentation generation 💡 Intelligent solution design suggestions 🎨 Streamlit-powered chat interface with dark theme ⚙️ Customizable model parameters (temperature, model size) 📚 Context-aware conversation history 🖥️ Local LLM deployment via Ollama Installation Prerequisites:\nOllama installed and running Python 3.9+ environment Clone the repository: bash git clone https://github.com/yourusername/deepseek-code-companion.git cd deepseek-code-companion\nInstall dependencies:\npip install -r requirements.txt Pull desired models (example for DeepSeek 1.5B): ollama pull deepseek-r1:1.5b Usage Start the Streamlit app: streamlit run app.py Configure settings in the sidebar:\nSelect model variant (1.5B, 3B, 32B) Adjust temperature for creativity control View model capabilities Interact with the chat interface:\nType coding questions or paste error messages Get AI-powered solutions with debugging support Clear chat history as needed Configuration Available Models Model Name Size Best For deepseek-r1:1.5b 1.5B Quick answers, basic code deepseek-r1:3b 3B Balanced performance deepseek-r1:32b 32B Complex problem solving llava:latest 7B Multimodal tasks llama3.2:latest 70B Advanced reasoning Temperature Guide Low (0.0-0.3): Factual, deterministic responses Medium (0.4-0.6): Balanced creativity High (0.7-1.0): Creative solutions, experimental code Technologies Used Streamlit: Web interface and chat management LangChain: LLM pipeline orchestration Ollama: Local LLM deployment and management DeepSeek Models: Specialized coding AI models Custom CSS: Styled chat interface and components Contributing Contributions are welcome! Please follow these steps:\nFork the repository Create your feature branch (git checkout -b feature/amazing-feature) Commit your changes (git commit -m 'Add some amazing feature') Push to the branch (git push origin feature/amazing-feature) Open a Pull Request License Distributed under the MIT License. See LICENSE for more information.\nAcknowledgements Ollama team for seamless local LLM management LangChain for LLM orchestration framework DeepSeek for their specialized coding models Streamlit for rapid UI development Note: Ensure Ollama server is running at http://localhost:11434 before starting the app. Custom CSS styling can be modified in the app.py header section.\n","permalink":"http://localhost:1313/project/llm/","summary":"\u003ch2 id=\"llm-model-a-game-changer-to-the-world-of-ai\"\u003eLLM Model a game changer to the world of AI\u003c/h2\u003e\n\u003ch1 id=\"llm_model\"\u003eLLM_model\u003c/h1\u003e\n\u003cp\u003eThis app is a dummy of the Big available LLM model using models like Ollama deepseek-r1:1.5b\u003c/p\u003e\n\u003ch1 id=\"-deepseek-code-companion\"\u003e🧠 DeepSeek Code Companion\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://streamlit.io/\"\u003e\u003cimg alt=\"Streamlit\" loading=\"lazy\" src=\"https://img.shields.io/badge/Streamlit-FF4B4B?logo=streamlit\u0026logoColor=white\"\u003e\u003c/a\u003e \u003ca href=\"https://python.langchain.com/\"\u003e\u003cimg alt=\"LangChain\" loading=\"lazy\" src=\"https://img.shields.io/badge/LangChain-00ADD8?logo=langchain\u0026logoColor=white\"\u003e\u003c/a\u003e\n\u003ca href=\"https://ollama.ai/\"\u003e\u003cimg alt=\"Ollama\" loading=\"lazy\" src=\"https://img.shields.io/badge/Ollama-FFFFFF?logo=ollama\u0026logoColor=black\"\u003e\u003c/a\u003e\n\u003ca href=\"https://opensource.org/licenses/MIT\"\u003e\u003cimg alt=\"License: MIT\" loading=\"lazy\" src=\"https://img.shields.io/badge/License-MIT-yellow.svg\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eYour AI-powered pair programmer with advanced debugging capabilities and code optimization features.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Demo Screenshot\" loading=\"lazy\" src=\"https://github.com/AnuragSharma5893/LLM_model/blob/main/ui%20(2).png?raw=true\"\u003e\u003c/p\u003e\n\u003ch2 id=\"features\"\u003eFeatures\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e🚀 Multi-model support (DeepSeek, LLaVA, Llama3)\u003c/li\u003e\n\u003cli\u003e🔥 Real-time code debugging assistance\u003c/li\u003e\n\u003cli\u003e📝 Automatic code documentation generation\u003c/li\u003e\n\u003cli\u003e💡 Intelligent solution design suggestions\u003c/li\u003e\n\u003cli\u003e🎨 Streamlit-powered chat interface with dark theme\u003c/li\u003e\n\u003cli\u003e⚙️ Customizable model parameters (temperature, model size)\u003c/li\u003e\n\u003cli\u003e📚 Context-aware conversation history\u003c/li\u003e\n\u003cli\u003e🖥️ Local LLM deployment via Ollama\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"installation\"\u003eInstallation\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003ePrerequisites\u003c/strong\u003e:\u003c/p\u003e","title":"Code Companion"},{"content":"Check Out my Blog Website to know more:- Link\nSum of Good Subsequences Approach to the Problem We are tasked with finding the sum of all possible good subsequences in a given array nums where a good subsequence satisfies the condition that the absolute difference between any two consecutive elements is exactly 1. Here\u0026rsquo;s how we can systematically solve this problem.\nUnderstand the Problem\nSubsequences: A subsequence is a sequence derived from an array by deleting some or no elements without changing the order of the remaining elements. Example: [1, 2, 1] has subsequences: [1], [2], [1], [1, 2], [2, 1], [1, 2, 1]. Good Subsequences: A subsequence is good if the absolute difference between consecutive elements is exactly 1. Goal: Compute the sum of all good subsequences modulo 10⁹+7. Brute-Force Approach Generating All the Subsequences: Use recursion or bitmasking to generate all possible subsequences of the array. Check if each subsequence is good by ensuring the condition ∣a[i]−a[i−1]∣ = 1 is satisfied for all consecutive elements. Sum up all elements of the good subsequences. Time Complexity: Generating all subsequences takes O(2^n), where n is the length of the array. Verifying each subsequence for the good property takes O(n) in the worst case. Overall: O(n⋅2^n). This is infeasible for large n (up to 10⁵). Optimal Approach To handle the constraints efficiently, we use a dynamic programming approach with optimization based on frequencies of numbers in the array. Algorithm We Use:-\nFrequency Array: Count the occurrences of each number in the array. This allows us to consider only the numbers present in the array, reducing unnecessary computation. Dynamic Programming Arrays: Using two arrays: sum[c]: Tracks the sum of elements in all good subsequences ending with number c. count[c]: Tracks the number of good subsequences ending with number c. Transition: For each number c in nums:\nAdd contributions from subsequences ending at c-1 (if c-1 exists in the array). Add contributions from subsequences ending at c+1 (if c+1 exists in the array). Add the number c itself as a single-element subsequence. Modulo: Since the answer can be large, perform all computations modulo 10⁹+7. Final Sum: Sum up all values in the sum array to get the result.\nint sumOfGoodSubsequences(int[] nums) { int MOD = 1e9 + 7; int MAX = 1e5 + 1; long[] sum = new long[MAX]; long[] count = new long[MAX]; int[] freq = new int[MAX]; // Count frequency of each number for (int num : nums) { freq[num]++; } for (int c = 0; c \u0026lt; MAX; c++) { if (freq[c] == 0) continue; if (c - 1 \u0026gt;= 0) { sum[c] += (count[c - 1] * c + sum[c - 1]) % MOD; count[c] += count[c - 1] % MOD; } sum[c] += c * freq[c]; count[c] += freq[c]; if (c + 1 \u0026lt; MAX) { sum[c] += (count[c + 1] * c + sum[c + 1]) % MOD; count[c] += count[c + 1] % MOD; } sum[c] %= MOD; count[c] %= MOD; } long result = 0; for (int c = 0; c \u0026lt; MAX; c++) { result = (result + sum[c]) % MOD; } return (int) result; } class Solution { public int sumOfGoodSubsequences(int[] nums) { long[] sum = new long[(int) (1e5 + 1)]; long[] count = new long[(int) (1e5 + 1)]; int[] freq = new int[(int) (1e5 + 1)]; long mod = (long) (1e9 + 7); for(int i: nums) freq[i]++; for(int i=0; i\u0026lt;nums.length; i++){ int c = nums[i]; if(c-1 \u0026gt;= 0 \u0026amp;\u0026amp; count[c-1] \u0026gt; 0){ sum[c] += (count[c-1] * c + sum[c-1]); count[c] += count[c-1]; } sum[c] += c; count[c] += 1; if(c+1 \u0026lt; sum.length \u0026amp;\u0026amp; count[c+1] \u0026gt; 0){ sum[c] += (count[c+1] * c + sum[c+1]); count[c] += count[c+1]; } sum[c] %= mod; count[c] %= mod; } long res = 0; for(int i=0; i \u0026lt; sum.length; ++i){ if (sum[i] \u0026gt; 0){ //system.out.println(i + \u0026#34;\u0026#34; + sum[i]); } res = (res % mod + sum[i] % mod) % mod; } return (int) (res % mod); } } Time Complexity Frequency Calculation: O(n), where n is the length of the array.\nDynamic Programming Update: O(range of numbers) = O(10⁵), as we iterate over the possible range of values.\nFinal Sum Computation: O(105)O(10⁵)O(105).\nOverall Time Complexity: O(n+10⁵)≈ O(n) for n≤10⁵\nSpace Complexity Frequency array: O(10⁵).\nDP arrays (sum and count): O(10⁵).\nOverall Space Complexity: O(10⁵).\nHow to Tackle Similar Problems\nBreak Down the Problem: Identify properties of valid subsequences (like the condition ∣a[i]−a[i−1]∣=1|a[i] — a[i-1]| = 1∣a[i]−a[i−1]∣=1). Use Frequency Arrays: Preprocess the array to avoid redundant computation. Dynamic Programming: Use DP to build solutions iteratively while maintaining the necessary conditions. Modulo Arithmetic: Always consider constraints like modulo 10⁹+7 for large results. ","permalink":"http://localhost:1313/blog/2024-11-17-sum-of-good-subsequences/","summary":"\u003cp\u003eCheck Out my Blog Website to know more:- \u003ca href=\"https://anuragsharma5893.github.io/\"\u003eLink\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"sum-of-good-subsequences\"\u003eSum of Good Subsequences\u003c/h1\u003e\n\u003cp\u003e\u003cimg alt=\"image1\" loading=\"lazy\" src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*N0YN64RifWvDOPuG3VXf7A.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"approach-to-the-problem\"\u003eApproach to the Problem\u003c/h2\u003e\n\u003cp\u003eWe are tasked with finding the sum of all possible good subsequences in a given array nums where a good subsequence satisfies the condition that the absolute difference between any two consecutive elements is exactly 1. Here\u0026rsquo;s how we can systematically solve this problem.\u003c/p\u003e\n\u003cp\u003eUnderstand the Problem\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eSubsequences:\nA subsequence is a sequence derived from an array by deleting some or no elements without changing the order of the remaining elements.\nExample: [1, 2, 1] has subsequences: [1], [2], [1], [1, 2], [2, 1], [1, 2, 1].\u003c/li\u003e\n\u003cli\u003eGood Subsequences:\nA subsequence is good if the absolute difference between consecutive elements is exactly 1.\u003c/li\u003e\n\u003cli\u003eGoal:\nCompute the sum of all good subsequences modulo 10⁹+7.\nBrute-Force Approach\u003c/li\u003e\n\u003cli\u003eGenerating All the Subsequences:\nUse recursion or bitmasking to generate all possible subsequences of the array.\nCheck if each subsequence is good by ensuring the condition ∣a[i]−a[i−1]∣ = 1 is satisfied for all consecutive elements.\nSum up all elements of the good subsequences.\u003c/li\u003e\n\u003cli\u003eTime Complexity:\nGenerating all subsequences takes O(2^n), where n is the length of the array.\nVerifying each subsequence for the good property takes O(n) in the worst case.\nOverall: O(n⋅2^n). This is infeasible for large n (up to 10⁵).\nOptimal Approach\nTo handle the constraints efficiently, we use a dynamic programming approach with optimization based on frequencies of numbers in the array.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eAlgorithm We Use:-\u003c/p\u003e","title":"3351. Sum of Good Subsequences"},{"content":"Check Out my Blog Website to know more:- Link\nHeight of Binary Tree After Subtree Removal Queries Leetcode 2458. Today in the daily Leetcode problem of the day, we have a hard-level question related to trees.\nBut first Let’s understand the question\nWe have a Binary Tree with n nodes with unique values from 1 to n.\nWe have given queries[i] of size n which are independent. We have to perform queries[i] such that we Remove the subtree rooted at the node with the value queries[i] from the tree. Remember, It is guaranteed that queries[i] will not be equal to the value of the root.\nConstraints:\n• The number of nodes in the tree is n. • 2 \u0026lt;= n \u0026lt;= 105 • 1 \u0026lt;= Node.val \u0026lt;= n • All the values in the tree are unique. • m == queries.length • 1 \u0026lt;= m \u0026lt;= min(n, 104) • 1 \u0026lt;= queries[i] \u0026lt;= n • queries[i] != root.val Our task is to Return an array answer of size m where answer[i] is the height of the tree after performing the ith query.\nInput: root = [1,3,4,2,null,6,5,null,null,null,null,null,7], queries = [4] Output: [2] Explanation: The diagram above shows the tree after removing the subtree rooted at node with value 4. The height of the tree is 2 (The path 1 -\u0026gt; 3 -\u0026gt; 2).\nLet’s perform queries by taking an example:-\nInput: root = [5,8,9,2,1,3,7,4,6], queries = [3,2,4,8] Output: [3,2,3,2]\nBasically, we have to see after removing the queries node our tree height is disturbing or not. For that, we will calculate the maximum depth of the tree from both the left and right sides of the tree.\nNow, Let\u0026rsquo;s see through right side\nNow we will put index 6 as max 2. And if we include 6 the height max. would be 3 Now, We will perform DFS traversal on both sides of the tree to perform the queries[i] by Remove and check whether after removing the form left can get the maximum height from the right side or vice versa just as mentioned in the problem statement.\nNow let\u0026rsquo;s write code in Java\nclass Solution { int max; public int[] treeQueries(TreeNode root, int[] queries) { int left[] = new int[100001]; int right[] = new int[100001]; max = 0; leftFirst(root, left, 0); max = 0; rightFirst(root, right, 0); for(int i=0; i\u0026lt;queries.length; i++){ queries[i] = Math.max(left[queries[i]], right[queries[i]]); } return queries; } private void leftFirst(TreeNode root, int[] left, int d) { if(root == null) return; left[root.val] = max; max = Math.max(max, d); // d = deapth leftFirst(root.left, left, d+1); leftFirst(root.right, left, d+1); } private void rightFirst(TreeNode root, int[] right, int d) { if(root == null) return; right[root.val] = max; max = Math.max(max, d); // d = deapth rightFirst(root.right, right, d+1); rightFirst(root.left, right, d+1); } } The Time and Space Complexity would be O(n).\n","permalink":"http://localhost:1313/blog/2024-10-26-height-of-the-binary-tree/","summary":"\u003cp\u003eCheck Out my Blog Website to know more:- \u003ca href=\"https://anuragsharma5893.github.io/\"\u003eLink\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"height-of-binary-tree-after-subtree-removal-queries\"\u003eHeight of Binary Tree After Subtree Removal Queries\u003c/h1\u003e\n\u003ch2 id=\"leetcode-2458\"\u003eLeetcode 2458.\u003c/h2\u003e\n\u003cp\u003eToday in the daily Leetcode problem of the day, we have a hard-level question related to trees.\u003c/p\u003e\n\u003cp\u003eBut first Let’s understand the question\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Question Picture\" loading=\"lazy\" src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*jPm63ps3dlD6O6nQBhw0gA.png\"\u003e\u003c/p\u003e\n\u003cp\u003eWe have a Binary Tree with n nodes with unique values from 1 to n.\u003c/p\u003e\n\u003cp\u003eWe have given queries[i] of size n which are independent. We have to perform queries[i] such that we Remove the subtree rooted at the node with the value queries[i] from the tree. Remember, It is guaranteed that queries[i] will not be equal to the value of the root.\u003c/p\u003e","title":"2458. Height of the Binary Tree"},{"content":"Check Out my Blog Website to know more:- Link\nMIT 6.S191 Introduction to Deep Learning MIT\u0026rsquo;s introductory program on deep learning methods with applications in language, medicine, art, computer vision, game play, robotics and more! old Edition 2024, 2025 Lecturer: Alexander Amini Lecture 1: Intro to Deep Learning Apr. 29, 2024\nDeep Learning – The Brainy Robot 🤖🧠 Imagine you have a robot friend who wants to learn how to recognize things, like cats, dogs, or even your handwriting. How does it learn?\nWell, we teach it just like a human brain learns! That’s what Deep Learning is all about.\n1. The Human Brain vs. Artificial Neural Networks 🧠 Your brain has neurons (tiny brain cells) that help you think and learn.\n🤖 Deep Learning uses \u0026ldquo;Artificial Neural Networks\u0026rdquo; (ANNs)—a bunch of connected math operations that act like neurons.\n🔗 Each neuron in the network:\nTakes in information (like a picture of a cat 🐱) Makes a small decision about it Passes the info to the next neuron The more neurons we have, the \u0026ldquo;deeper\u0026rdquo; the network is. That’s why we call it Deep Learning!\n2. How Deep Learning Learns (Like a Kid!) Imagine you\u0026rsquo;re a kid learning to recognize cats and dogs.\nYou see a picture of a cat. 🐱 Someone tells you, \u0026ldquo;That’s a cat!\u0026rdquo; You remember it. ✅ Next time, you see another animal and guess\u0026hellip; If you’re right → Your brain strengthens the memory! If you’re wrong → You learn from the mistake and try again. Deep Learning works exactly the same way but much, much faster! 🚀\n3. Why \u0026ldquo;Deep\u0026rdquo;? Layers of Learning! Think of Deep Learning like a big sandwich 🥪, where each layer adds new knowledge:\nFirst Layer: Looks at basic features (lines, edges, colors) Middle Layers: Recognizes patterns (ears, eyes, tails) Final Layer: Understands the full picture (\u0026ldquo;Aha! It’s a cat!\u0026rdquo;) The more layers, the smarter the AI becomes!\n4. What Can Deep Learning Do? Deep Learning helps with tons of cool things:\n✅ Recognizing faces in photos 📸\n✅ Understanding speech (like Alexa, Siri) 🎤\n✅ Translating languages 🌍\n✅ Playing games (like Chess \u0026amp; Go) 🎮\n✅ Driving self-driving cars 🚗\nThat’s how AI can see, hear, and even talk like us! 🤖✨\nWatch Video Lecture Lecture 2: Deep Sequence Modeling May 6, 2024\nRecurrent Neural Networks, Transformers, and Attention {: .prompt-info}\nAlright! Imagine you\u0026rsquo;re in a classroom with a really smart robot teacher that helps you remember and understand things.\n1. Recurrent Neural Networks (RNNs) – The Storyteller Think of RNNs like a storyteller who remembers the previous parts of a story while telling the next part.\n📖 Imagine you\u0026rsquo;re reading a storybook one page at a time.\nTo understand the current page, you need to remember what happened on the previous pages. RNNs work the same way—they remember past words when predicting the next one. But there\u0026rsquo;s a problem! 😟\nIf the story is too long, the robot forgets the beginning. (Like when you forget what happened in Chapter 1 of a long book!) 2. Transformers – The Super Smart Librarian Since RNNs forget things, Transformers were invented to read the whole book at once instead of page by page!\n📚 Imagine you have a librarian who doesn’t just read a book page by page but instead:\nOpens the whole book at once Finds the most important parts instantly Uses those parts to understand the story better This means Transformers don’t forget things easily! That’s why they power tools like ChatGPT, Google Translate, and Siri!\n3. Attention – The Magic Highlighter Now, how does the librarian (Transformer) know which parts of the book are important?\n🖍 Think of Attention like a magic highlighter ✨\nInstead of reading everything equally, the librarian highlights the most important words. For example, if the sentence is:\n👉 \u0026ldquo;I went to the zoo and saw a lion.\u0026rdquo; The most important word for \u0026ldquo;What animal did I see?\u0026rdquo; is \u0026ldquo;lion\u0026rdquo; Attention helps the Transformer focus on \u0026ldquo;lion\u0026rdquo; and ignore unnecessary words! And that’s how modern AI understands language! 🤖🚀\nWatch Video Lecture\nLecture 3: Intro to TensorFlow - Music Generation Software Lab 1\nMIT 6.S191 Lab 1: Intro to Deep Learning in Python and Music Generation with RNNs Part 1: Intro to Deep Learning in Python \u0026ndash; TensorFlow and PyTorch TensorFlow (\u0026ldquo;TF\u0026rdquo;) and PyTorch (\u0026ldquo;PT\u0026rdquo;) are software libraries used in machine learning. Here we\u0026rsquo;ll learn how computations are represented and how to define simple neural networks in TensorFlow and PyTorch. The TensorFlow labs will be prefixed by TF; PyTorch labs will be prefixed by PT.\nTensorFlow uses a high-level API called Keras that provides a powerful, intuitive framework for building and training deep learning models. In the TensorFlow Intro (TF_Part1_Intro) you will learn the basics of computations in TensorFlow, the Keras API, and TensorFlow 2.0\u0026rsquo;s imperative execution style.\nPyTorch is a popular deep learning library known for its flexibility, ease of use, and dynamic execution. In the PyTorch Intro (PT_Part1_Intro) you will learn the basics of computations in PyTorch and how to define neural networks using either the sequential API and torch.nn.Module.\nPart 2: Music Generation with RNNs In the second portion of the lab, we will play around with building a Recurrent Neural Network (RNN) for music generation. We will be using a \u0026ldquo;character RNN\u0026rdquo; to predict the next character of sheet music in ABC notation. Finally, we will sample from this model to generate a brand new music file that has never been heard before!\nLecture 4: Deep Computer Vision May 13, 2024\nWatch Video Lecture Convolutional Neural Networks (CNNs) – The AI That Sees! 👀🤖 Imagine you have a robot friend 🦾 that wants to see and understand pictures—like recognizing cats, dogs, or even your face in a selfie!\nThat’s what Convolutional Neural Networks (CNNs) do! They are super smart at looking at images, just like our eyes and brain.\n1. Why Do We Need CNNs? Regular neural networks struggle with images because pictures have millions of pixels! 😵\n👀 Example:\nA small image (100x100 pixels) has 10,000 pixels!\nA normal neural network would treat each pixel separately (very slow! 🐢). A CNN is smarter—it looks for patterns like edges, shapes, and textures, just like how humans recognize things! 2. How CNNs Work (Like a Detective!) 🔍 Step 1: Convolution – Finding Features Think of CNNs like a detective with a magnifying glass 🔎.\nIt scans small parts of an image piece by piece instead of looking at everything at once. It finds edges, curves, and colors (like eyes, noses, or ears in a face). 🔲 Example: If you\u0026rsquo;re looking for a cat 🐱, CNN first finds:\n✔ Whiskers\n✔ Ears\n✔ Round face\nEach part helps build the full picture!\nStep 2: Pooling – Making It Simpler CNNs then shrink the image while keeping important details.\nThis makes processing faster and removes unnecessary noise. Think of it like zooming out on a blurry photo but still knowing it\u0026rsquo;s a cat! 🖼️ Step 3: Fully Connected Layer – The Final Guess! After finding all features, CNN connects the dots and makes a final decision.\n🧠 Example:\n\u0026ldquo;Does this image have whiskers? ✅\u0026rdquo; \u0026ldquo;Pointy ears? ✅\u0026rdquo; \u0026ldquo;A cute nose? ✅\u0026rdquo;\n👉 It must be a cat! 🐱 3. What Can CNNs Do? 🤯 CNNs are used in tons of cool applications:\n✅ Face Recognition – Unlock your phone with Face ID 📸\n✅ Self-Driving Cars – Detecting traffic signs \u0026amp; pedestrians 🚗\n✅ Medical Diagnosis – Identifying diseases in X-rays \u0026amp; MRIs 🏥\n✅ Security Cameras – Detecting suspicious activity 🚨\nLecture 5: Deep Generative Modeling May 20, 2024\nWatch Video Lecture\nDeep Generative Modeling – The AI That Creates! 🎨🤖 Imagine you have a magic artist robot 🦾🎨 that can create new pictures, music, or even human faces—things that never existed before!\nThat’s what Deep Generative Models do. They learn patterns from real data and then generate new, similar data—just like how a human artist learns to paint by studying other paintings.\n1. What Does \u0026ldquo;Generative\u0026rdquo; Mean? 🔹 \u0026ldquo;Generative\u0026rdquo; means AI can create new things instead of just recognizing them.\n🔹 Instead of just looking at cat pictures 🐱 and saying \u0026ldquo;This is a cat,\u0026rdquo; it can actually generate a brand-new cat picture that has never been seen before!\n2. Types of Generative Models There are two main types of Deep Generative Models:\n1️⃣ Generative Adversarial Networks (GANs) – The Artist \u0026amp; The Critic 🎭 Think of GANs like a game between:\n🎨 An Artist (Generator) – Tries to create fake images.\n👀 A Critic (Discriminator) – Tries to catch fake images and say, \u0026ldquo;Nope, that’s not real!\u0026rdquo;\n🤖 How It Works:\nThe Artist (Generator) starts by making bad drawings (random noise 🎨). The Critic (Discriminator) checks if they look real. If the drawing is bad, the artist improves. If it\u0026rsquo;s good, the critic gets tricked! 😲 This battle repeats thousands of times until the Artist creates images that look real! ✅ GANs are used for:\nCreating realistic human faces (like deepfake videos 🎭) Making new paintings in the style of famous artists 🎨 Enhancing old photos (turning blurry images into HD) 2️⃣ Variational Autoencoders (VAEs) – The Smart Copy Machine 📄🤖 A VAE is like a super-smart copy machine that:\nLearns to understand what an image looks like. Creates a slightly different version from scratch. 🤖 Example:\nIt studies hundreds of dog pictures 🐶 Then, it can imagine a new dog that doesn’t exist in real life! ✅ VAEs are used for:\nGenerating new designs \u0026amp; sketches Creating new human faces or fashion models Generating synthetic medical data (for privacy-friendly AI training) 3. What Can Deep Generative Models Do? 🤯 Deep Generative Models are everywhere:\n✅ AI Art – Creates beautiful paintings 🎨\n✅ Deepfake Videos – Makes people say things they never said 🎭\n✅ Text-to-Image Models – Like DALL·E (turns words into pictures 🖼️)\n✅ AI Music – Composes original songs 🎵\n✅ Medical Image Generation – Generates fake X-rays for research 🏥\nNow, AI isn’t just thinking—it’s creating! 🎨🤖🚀\nLecture 6: Facial Detection Systems Facial Detection Systems Research Paper, Code {: .prompt-info}\nMnist_2layers_arch true\nmnist_model convnet_ SS_VAE DB_VAE Lecture 7: Deep Reinforcement Learning May 27, 2024\nWatch Video Lecture\nDeep Reinforcement Learning – The AI That Learns by Playing! 🎮🤖 Imagine you have a robot student 🤖📚 that wants to learn how to play a game, drive a car, or even walk like a human.\nInstead of just memorizing answers, it learns by trying, making mistakes, and improving—just like how you learned to ride a bike 🚴‍♂️!\nThat’s Deep Reinforcement Learning (DRL)!\n1. What is Reinforcement Learning? Reinforcement Learning (RL) is like teaching a dog tricks 🐶🎾.\n1️⃣ You give it a command (\u0026ldquo;Sit!\u0026rdquo;)\n2️⃣ If it sits correctly, you give it a treat 🍖 (reward ✅)\n3️⃣ If it doesn’t, you give no treat ❌\n4️⃣ Over time, the dog learns what gets rewards and what doesn’t!\nDeep RL does the same thing but with a computer brain (AI)!\n2. The Key Players in Reinforcement Learning 🎭 Every RL system has 3 main parts:\n1️⃣ The Agent (The Learner) 🤖 This is the AI that is trying to learn—like a robot playing a video game.\n2️⃣ The Environment (The World) 🌍 This is the place where the agent learns, like a game world, a self-driving car simulation, or a chessboard.\n3️⃣ Rewards (The Prize 🏆) The AI gets a reward for making good decisions and a penalty for making bad ones.\n🤖 Example:\nIf a self-driving car stays in the lane → ✅ Reward! If it crashes into a tree → ❌ Penalty! Over time, the AI figures out how to win by choosing actions that give the best rewards!\n3. How Deep Reinforcement Learning Works 🧠💡 Regular RL is too slow for complex problems (like playing chess or driving a car). That’s why we add Deep Learning to make it smarter!\n🔹 Instead of guessing randomly, Deep RL uses Neural Networks to:\n1️⃣ Remember past experiences 📚\n2️⃣ Predict the best moves in the future 🔮\n3️⃣ Improve itself over time ⏳\n4. Cool Applications of Deep RL 🚀 🔥 AI Playing Video Games 🎮 – AlphaGo, DeepMind’s AI, defeated the world champion in Go!\n🚗 Self-Driving Cars 🚘 – Learning to drive by practicing in simulations.\n🤖 Robots Learning to Walk 🚶 – Teaching AI to move like humans!\n📊 Stock Market Trading 📈 – AI learns to invest money smartly.\n5. Summary 🔹 Deep RL is like training a dog – good actions get rewards, bad actions get penalties.\n🔹 It has an Agent (Learner), an Environment (World), and Rewards (Prizes).\n🔹 Deep Learning helps it learn faster and better!\n🔹 Used in video games, self-driving cars, robotics, and finance!\nDeep RL teaches AI to learn from experience—just like humans do! 🤖🎮🚀\nLecture 8: Language Models and New Frontiers June 3, 2024\nLanguage models are AI systems trained to understand and generate human-like text. Traditional models, like RNNs, struggled with long-term memory, but Transformers revolutionized NLP with self-attention mechanisms, leading to breakthroughs like GPT and BERT. These models power chatbots, translation tools, and content generation. The frontier is expanding with multimodal AI (text + images + speech), retrieval-augmented models, and AI agents that reason and plan. Ethical concerns around bias, misinformation, and AI alignment remain critical as we push toward more advanced, human-like intelligence. 🚀\nWatch Lecture online\nLecture 9: Generative AI for Media June 10, 2024\nLecturer: Doug Eck (Google). Douglas Eck is a Senior Research Director at Google, and leads research efforts at Google DeepMind in Generative Media, including image, video, 3D, music, and audio generation.\nLecture 10: Stories from Models in the Wild June 17, 2024\nEndless Experimentation: Building AI Models in the Wild. Lecturer: Niko Laskaris (VP, Customer Engineering) and Doug Blank (Head of Research) Comet ML\n","permalink":"http://localhost:1313/blog/2025-02-05-mit-deep-learning/","summary":"\u003cp\u003eCheck Out my Blog Website to know more:- \u003ca href=\"https://anuragsharma5893.github.io/\"\u003eLink\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"mit-6s191-introduction-to-deep-learning\"\u003eMIT 6.S191 Introduction to Deep Learning\u003c/h1\u003e\n\u003ch4 id=\"mits-introductory-program-on-deep-learning-methods-with-applications-in-language-medicine-art-computer-vision-game-play-robotics-and-more\"\u003eMIT\u0026rsquo;s introductory program on deep learning methods with applications in language, medicine, art, computer vision, game play, robotics and more!\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eold Edition \u003ca href=\"https://introtodeeplearning.com/2024/index.html\"\u003e2024\u003c/a\u003e, \u003ca href=\"https://introtodeeplearning.com/\"\u003e2025\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eLecturer: Alexander Amini\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"lecture-1-intro-to-deep-learning\"\u003eLecture 1: Intro to Deep Learning\u003c/h2\u003e\n\u003cp\u003eApr. 29, 2024\u003c/p\u003e\n\u003ch4 id=\"deep-learning--the-brainy-robot-\"\u003e\u003cstrong\u003eDeep Learning – The Brainy Robot\u003c/strong\u003e 🤖🧠\u003c/h4\u003e\n\u003cp\u003eImagine you have a \u003cstrong\u003erobot friend\u003c/strong\u003e who wants to learn how to recognize things, like cats, dogs, or even your handwriting. How does it learn?\u003c/p\u003e","title":"MIT 6.S191 Introduction to Deep Learning"},{"content":"Check Out my Blog Website to know more:- Link\nMIT 9.13 The Human Brain, Spring 2019 Instructor: Nancy Kanwisher\nView the complete course\nLecture 1: Introduction Download the PDF of Lecture01 Prof. Kanwisher tells a true story to introduce the course, then covers the why, how, and what of studying the human brain and gives a course overview. {: .prompt-info}\nWatch the Lecture Video Lecture 2: Neuroanotomy Download the PDF Basic brief neuroanatomy review in preparation for dissection, including an introduction to the cortex, primary regions, and topographic maps. {: .prompt-info}\nWatch the Video Lecture Lecture 4 : Cognitive Neuroscience Methods I Download the PDF Introduction to methods in cognitive neuroscience including computation, behavior, fMRI, ERPs \u0026amp; MEG, neuropsychology patients, TMS, and intracranial recordings in humans and nonhuman primates. {: .prompt-info}\nWatch Video Lecture Lecture 5: Cognitive Neuroscience Methods II Download the PDF Methods in cognitive neuroscience continued. {: .prompt-info}\nWatch the Video Lecture Lecture 6: Introduction to the Human Brain Download the PDF This session reviews the last two lectures, face recognition, and explores types of experimental methods. {: .prompt-info}\nWatch the video Lecture Lecture 7: Category Selectivity, Controversies, and MVPA Download the PDF Covers controversies and alternative views of the ventral visual pathway, multiple voxel pattern analysis, and the two visual pathways. {: .prompt-info}\nWatch the Video Lecture here Lecture 8: Navigation I Download the PDF The functional organization of scene perception and navigation and the various brain structures that implement them. {: .prompt-info}\nWatch the Video Lecture Lecture 9: Navigation II Download the PDF Scene perception and navigation continued. {: .prompt-info}\nWatch Lecture_09 Lecture 10: Development, Nature \u0026amp; Nurture I Download the PDF This lecture examines how we think the cortex organizes in the brain over infancy and childhood, and the function of genes vs experience. {: .prompt-tip}\nWatch Video Lecture Lecture 11: Development, Nature \u0026amp; Nurture II (2018) Download the PDF Continues the discussion of genes vs experience on cortical organization, and whether the cortex can change in adulthood. {: .prompt-tip}\nWatch Video Lecture Lecture 13: Number Download the PDF Explores the nature of the human representation of number and how it is implemented in the brain. NOTE: Lecture 14: New Methods Applied to Number (student breakout groups—video not recorded) {: .prompt-info}\nWatch Video Lecture. Lecture 15: Hearing and Speech Download the PDF Humans use hearing in species-specific ways, for speech and music. Ongoing research is working out the functional organization of these and other human auditory skills. {: .prompt-info}\nWatch Video Lecture online Lecture 16: Music Download the PDF The functional organization of music in human beings. NOTE: Lecture 17: MEG Decoding and RSA (video not recorded) {: .prompt-info}\nWatch Lecture on \"Music\" Lecture 18: Language I Download the PDF Covers the basic organization of language in the brain and the long-standing question of the relationship between thought and language.\nNOTE: Lecture 17: MEG Decoding and RSA (video not recorded) NOTE: Lecture 19: Language II (class canceled—video not recorded) {: .prompt-info} Video Lecture Lecture 20: Theory of Mind \u0026amp; Mentalizing Download the PDF The ability of humans to think about what other people are thinking is implemented in brain regions highly specialized for this function alone.\nNOTE: Lecture 19: Language II (class canceled—video not recorded) {: .prompt-info}\nWatch Lecture online Lecture 21: Brain Networks Download the PDF Looks at the major white matter tracts in the human brain, predicting function and correlations between regions.\nNOTE: Lecture 22: Experimental Design (student breakout groups—video not recorded) NOTE: Lecture 23: Deep Networks (2021) (video will be added soon) {: .prompt-tip}\nWatch Video lectures Lecture 24: Attention and Awareness Download the PDF Looks at the differences in the mind and brain between perceptual information we are aware of versus information we are not. NOTE: Lecture 23: Deep Networks (2021) (video will be added soon) {: .prompt-tip}\nWatch Video Lecture online ","permalink":"http://localhost:1313/blog/2025-02-04-notes-human-brain/","summary":"\u003cp\u003eCheck Out my Blog Website to know more:- \u003ca href=\"https://anuragsharma5893.github.io/\"\u003eLink\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"mit-913-the-human-brain-spring-2019\"\u003eMIT 9.13 The Human Brain, Spring 2019\u003c/h2\u003e\n\u003cp\u003eInstructor: Nancy Kanwisher\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://ocw.mit.edu/9-13S19\"\u003eView the complete course\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.youtube.com/playlist?list=PLUl4u3cNGP60IKRN_pFptIBxeiMc0MCJP\"\u003e\u003cimg alt=\"Playlist\" loading=\"lazy\" src=\"https://img.shields.io/badge/YouTube-FF0000?style=for-the-badge\u0026logo=youtube\u0026logoColor=white\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"lecture-1-introduction\"\u003eLecture 1: Introduction\u003c/h1\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003ca href=\"/assets/Notes/MIT9_13S19_L01.pdf\"\u003eDownload the PDF of Lecture01\u003c/a\u003e\nProf. Kanwisher tells a true story to introduce the course, then covers the why, how, and what of studying the human brain and gives a course overview.\n{: .prompt-info}\u003c/p\u003e\u003c/blockquote\u003e\n\u003ciframe src=\"{{ site.baseurl }}/assets/Notes/MIT9_13S19_L01.pdf\" width=\"90%\" height=\"500px\"\u003e\u003c/iframe\u003e\nWatch the Lecture Video  \n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/ba-HMvDn_vU?si=Oj1QIXPjO_jeVGQS\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen\u003e\u003c/iframe\u003e\n\u003ch1 id=\"lecture-2-neuroanotomy\"\u003eLecture 2: Neuroanotomy\u003c/h1\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003ca href=\"/assets/Notes/MIT9_13S19_L02.pdf\"\u003eDownload the PDF\u003c/a\u003e\nBasic brief neuroanatomy review in preparation for dissection, including an introduction to the cortex, primary regions, and topographic maps.\n{: .prompt-info}\u003c/p\u003e","title":"The Human Brain"},{"content":" University:- Indira Gandhi National Open University\nLink\nYear:- 2022- Present\nMy Journey to Psychology as a Undergrad CS major By the time I was in my second year of engineering, I had completely lost interest in it. But, strangely enough, it was anime that reignited my curiosity and led me down an unexpected path. 🎌✨\nI decided to enroll in a B.A. (Hons) in Psychology through an open distance learning university. People often asked me, \u0026ldquo;Why psychology?\u0026quot;—but I never had a perfect answer. The truth was simple: I genuinely enjoyed studying it. 🧠📚\nAfter a year of diving into cognitive science, human behavior, and mental processes, I realized that this wasn’t just a passing interest—I wanted to explore this field further. More importantly, I saw an exciting opportunity to bridge AI and human cognition, combining my knowledge of both to contribute to research at the intersection of technology and the human mind. 🚀🔬\n","permalink":"http://localhost:1313/education/psychology/","summary":"\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUniversity:- \u003ca href=\"https://en.wikipedia.org/wiki/Indira_Gandhi_National_Open_University\"\u003eIndira Gandhi National Open University\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://www.ignou.ac.in/\"\u003eLink\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eYear:- 2022- Present\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"my-journey-to-psychology-as-a-undergrad-cs-major\"\u003eMy Journey to Psychology as a Undergrad CS major\u003c/h1\u003e\n\u003cp\u003eBy the time I was in my \u003cstrong\u003esecond year of engineering\u003c/strong\u003e, I had completely lost interest in it. But, strangely enough, it was \u003cstrong\u003eanime\u003c/strong\u003e that reignited my curiosity and led me down an unexpected path. 🎌✨\u003c/p\u003e\n\u003cp\u003eI decided to \u003cstrong\u003eenroll in a B.A. (Hons) in Psychology\u003c/strong\u003e through an \u003cstrong\u003eopen distance learning university\u003c/strong\u003e. People often asked me, \u003cem\u003e\u0026ldquo;Why psychology?\u0026quot;\u003c/em\u003e—but I never had a perfect answer. The truth was simple: \u003cstrong\u003eI genuinely enjoyed studying it\u003c/strong\u003e. 🧠📚\u003c/p\u003e","title":"B.A.(Hons) Psychology 🧠"},{"content":" University:- Dr. A.P.J. Abdul Kalam Technical University, Lucknow\nCollege:- Mahatma Gandhi Mission\u0026rsquo;s College Of Engineering \u0026amp; Technology, Noida Year:- 2021-2025\nWhy CS major \u0026ldquo;Why Computer Science?\u0026rdquo;—a question I’ve been asked countless times. And my answer is always the same: \u0026ldquo;I\u0026rsquo;ve never done anything outside of this.\u0026rdquo; 💻🚀\nFrom the moment my father bought a computer for me and my brother, I was hooked. The first thing I did? Opened it up to see what was inside! 🔧🔍 I can still vividly remember the DDR1 RAM, barely 900MB, yet we spent hours playing games and experimenting with it.\nEven back then, I was confident that the world was heading towards a computer-driven future. That belief became my biggest motivation for choosing Computer Science \u0026amp; Engineering—not just as a field of study but as a way of life. 🖥️🔥\n","permalink":"http://localhost:1313/education/engineering/","summary":"\u003cblockquote\u003e\n\u003cp\u003eUniversity:- \u003ca href=\"https://en.wikipedia.org/wiki/Dr._A._P._J._Abdul_Kalam_Technical_University,_Lucknow\"\u003eDr. A.P.J. Abdul Kalam Technical University, Lucknow\u003c/a\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003cul\u003e\n\u003cli\u003eCollege:- Mahatma Gandhi Mission\u0026rsquo;s College Of Engineering \u0026amp; Technology, Noida\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp\u003eYear:- 2021-2025\u003c/p\u003e\u003c/blockquote\u003e\n\u003ch1 id=\"why-cs-major\"\u003eWhy CS major\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003e\u0026ldquo;Why Computer Science?\u0026rdquo;\u003c/strong\u003e—a question I’ve been asked countless times. And my answer is always the same: \u003cstrong\u003e\u0026ldquo;I\u0026rsquo;ve never done anything outside of this.\u0026rdquo;\u003c/strong\u003e 💻🚀\u003c/p\u003e\n\u003cp\u003eFrom the moment my father bought a \u003cstrong\u003ecomputer\u003c/strong\u003e for me and my brother, I was \u003cstrong\u003ehooked\u003c/strong\u003e. The first thing I did? \u003cstrong\u003eOpened it up\u003c/strong\u003e to see what was inside! 🔧🔍 I can still vividly remember the \u003cstrong\u003eDDR1 RAM\u003c/strong\u003e, barely \u003cstrong\u003e900MB\u003c/strong\u003e, yet we spent hours playing games and experimenting with it.\u003c/p\u003e","title":"Computer Science \u0026 Engineering 👨‍💻"},{"content":"📬 Get in Touch Looking forward to connecting with you! Whether you have questions, collaboration ideas, or just want to say hi, feel free to reach out through any of the platforms below:\n📱 Connect with Me 📧 Email: Share your Ideas with me 💻 GitHub: See my Projects \u0026amp; Some other cool stuff 💼 LinkedIn: linkedin.com/in/yourusername 🐦 Twitter/X: More Active here 📙 Substack: My Thoughtful Ideas 🌲 LinkTree: https://linktr.ee/Sharma_Anurag 💻 Blog: Check Out my Blog Website: Link ✉️ Send Me a Message Fill out the form below, and I\u0026rsquo;ll get back to you as soon as possible!\nLoading…\r🚀 Fill Out the Form\r✨ Looking forward to hearing from you!\n","permalink":"http://localhost:1313/contact/contact/","summary":"\u003ch1 id=\"-get-in-touch\"\u003e📬 Get in Touch\u003c/h1\u003e\n\u003cp\u003eLooking forward to connecting with you! Whether you have questions, collaboration ideas, or just want to say hi, feel free to reach out through any of the platforms below:\u003c/p\u003e\n\u003ch2 id=\"-connect-with-me\"\u003e📱 Connect with Me\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e📧 \u003cstrong\u003eEmail:\u003c/strong\u003e \u003ca href=\"mailto:anuragsharma58693@gmail.com\"\u003eShare your Ideas with me\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e💻 \u003cstrong\u003eGitHub:\u003c/strong\u003e \u003ca href=\"https://github.com/AnuragSharma5893\"\u003eSee my Projects \u0026amp; Some other cool stuff\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e💼 \u003cstrong\u003eLinkedIn:\u003c/strong\u003e \u003ca href=\"https://www.linkedin.com/in/anurag-sharma-6aa7ab216/\"\u003elinkedin.com/in/yourusername\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e🐦 \u003cstrong\u003eTwitter/X:\u003c/strong\u003e \u003ca href=\"https://x.com/anu0x7D4\"\u003eMore Active here\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e📙 \u003cstrong\u003eSubstack:\u003c/strong\u003e \u003ca href=\"https://substack.com/@hakunamatataa\"\u003eMy Thoughtful  Ideas\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e🌲 \u003cstrong\u003eLinkTree:\u003c/strong\u003e \u003ca href=\"https://linktr.ee/Sharma_Anurag\"\u003ehttps://linktr.ee/Sharma_Anurag\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e💻 \u003cstrong\u003eBlog:\u003c/strong\u003e Check Out my Blog Website: \u003ca href=\"https://anuragsharma5893.github.io/\"\u003eLink\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"-send-me-a-message\"\u003e✉️ Send Me a Message\u003c/h2\u003e\n\u003cp\u003eFill out the form below, and I\u0026rsquo;ll get back to you as soon as possible!\u003c/p\u003e","title":"Contact Me👋"}]